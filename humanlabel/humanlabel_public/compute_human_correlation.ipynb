{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute human-algorithm correlation\n",
    "\n",
    "* 사람실험 결과와 알고리즘 출력 간의 correlation을 계산한다.\n",
    "* 데이터 자체에 대한 분석은 별도의 노트북인 `human_label.ipynb` 를 참고한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-17T04:15:11.553833Z",
     "iopub.status.busy": "2021-03-17T04:15:11.553214Z",
     "iopub.status.idle": "2021-03-17T04:15:11.971964Z",
     "shell.execute_reply": "2021-03-17T04:15:11.971239Z",
     "shell.execute_reply.started": "2021-03-17T04:15:11.553758Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-17T04:19:15.744452Z",
     "iopub.status.busy": "2021-03-17T04:19:15.743820Z",
     "iopub.status.idle": "2021-03-17T04:19:15.780127Z",
     "shell.execute_reply": "2021-03-17T04:19:15.778797Z",
     "shell.execute_reply.started": "2021-03-17T04:19:15.744377Z"
    }
   },
   "outputs": [],
   "source": [
    "df_result = pd.read_csv('anon_results.csv')\n",
    "df_result.index.name = 'id'\n",
    "df_result = df_result.reset_index()#.rename(columns={df_result.index.name: 'id'})\n",
    "df_triplet = pd.read_csv('triplets.csv')\n",
    "df_triplet = df_triplet.set_index('triplet_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-17T04:19:18.166269Z",
     "iopub.status.busy": "2021-03-17T04:19:18.165691Z",
     "iopub.status.idle": "2021-03-17T04:19:18.192162Z",
     "shell.execute_reply": "2021-03-17T04:19:18.190838Z",
     "shell.execute_reply.started": "2021-03-17T04:19:18.166196Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "user_5     1753\n",
       "user_8     1618\n",
       "user_1     1518\n",
       "user_0     1239\n",
       "user_22    1029\n",
       "user_38    1007\n",
       "user_15     604\n",
       "user_37     407\n",
       "user_19     190\n",
       "user_34     188\n",
       "user_16     170\n",
       "user_40     109\n",
       "user_41     100\n",
       "user_35      88\n",
       "user_25      86\n",
       "user_6       72\n",
       "user_2       68\n",
       "user_9       67\n",
       "user_4       46\n",
       "user_7       45\n",
       "user_3       40\n",
       "user_20      37\n",
       "user_18      35\n",
       "user_11      35\n",
       "user_28      34\n",
       "user_24      21\n",
       "user_13      20\n",
       "user_26      14\n",
       "user_30      11\n",
       "user_36       9\n",
       "user_39       8\n",
       "user_21       8\n",
       "user_32       7\n",
       "user_23       6\n",
       "user_10       5\n",
       "user_27       5\n",
       "user_33       3\n",
       "user_12       3\n",
       "user_29       2\n",
       "user_17       2\n",
       "user_31       2\n",
       "user_14       1\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''users and the numers of their labels'''\n",
    "df_result.groupby('user_id').count()['id'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-17T04:19:26.566797Z",
     "iopub.status.busy": "2021-03-17T04:19:26.566180Z",
     "iopub.status.idle": "2021-03-17T04:19:26.588312Z",
     "shell.execute_reply": "2021-03-17T04:19:26.586923Z",
     "shell.execute_reply.started": "2021-03-17T04:19:26.566722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user_0', 'user_1', 'user_11', 'user_13', 'user_15', 'user_16', 'user_18', 'user_19', 'user_2', 'user_20', 'user_22', 'user_24', 'user_25', 'user_26', 'user_28', 'user_3', 'user_30', 'user_34', 'user_35', 'user_37', 'user_38', 'user_4', 'user_40', 'user_41', 'user_5', 'user_6', 'user_7', 'user_8', 'user_9']\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "'''filter users with low answer count (<10)'''\n",
    "filter_count = 10\n",
    "user_count = df_result.groupby('user_id').count()['id']\n",
    "l_filtered_users = (user_count[user_count > filter_count]).index.to_list()\n",
    "print(l_filtered_users)\n",
    "print(len(l_filtered_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-17T04:19:34.399172Z",
     "iopub.status.busy": "2021-03-17T04:19:34.398556Z",
     "iopub.status.idle": "2021-03-17T04:19:34.409100Z",
     "shell.execute_reply": "2021-03-17T04:19:34.407734Z",
     "shell.execute_reply.started": "2021-03-17T04:19:34.399097Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1752"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_triplet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inter-user agreement score for each user\n",
    "\n",
    "### Agreement metric from (Grodo and Larlus, 2017)\n",
    "For each triplet,\n",
    "\n",
    "$o1, o2$: the number of times the first (resp. second) image was chosen.  \n",
    "$o3$: the number of times people did not pick any of the two images.  \n",
    "\n",
    "Agreement score is computed as \n",
    "\n",
    "$$\n",
    "s = (\\frac{o_1+o_2 - 1}{o_1 + o_2 + o_3 - 1}) (\\frac{o_i - 1}{o_1 + o_2 - 1})\n",
    "$$\n",
    "where $o_i$ $i\\in\\{1,2\\}$ is the choice of the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our agreement metric\n",
    "Let  \n",
    "$o1, o2$ : the number of times the first (resp. second) image was chosen.  \n",
    "$p$ : the number of times both of images were chosen  \n",
    "$q$ : the nubmer of times neither of images were chosen\n",
    "\n",
    "\n",
    "If a person choses the first or the second image,  \n",
    "$$\n",
    "s = \\frac{o_i + 0.5 p - 1}{o_1 + o2 + p + q - 1}\n",
    "$$\n",
    "where $o_i$ $i\\in\\{1,2\\}$ is the choice of the user.\n",
    "\n",
    "If a person choses the \"both\" option,\n",
    "$$\n",
    "s = \\frac{0.5o_1 + 0.5o_2 + p - 1}{o_1 + o_2 + p + q - 1}\n",
    "$$\n",
    "\n",
    "If a person choses the \"neither\" options,\n",
    "$$\n",
    "s = 0\n",
    "\n",
    "$$\n",
    "\n",
    "Given a person, the score is averaged over the whole triplet which he/she labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-17T04:19:38.406966Z",
     "iopub.status.busy": "2021-03-17T04:19:38.406368Z",
     "iopub.status.idle": "2021-03-17T04:19:38.479484Z",
     "shell.execute_reply": "2021-03-17T04:19:38.478493Z",
     "shell.execute_reply.started": "2021-03-17T04:19:38.406891Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>answer</th>\n",
       "      <th>o1</th>\n",
       "      <th>o2</th>\n",
       "      <th>both</th>\n",
       "      <th>neither</th>\n",
       "      <th>o3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>triplet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "answer       o1   o2  both  neither   o3\n",
       "triplet_id                              \n",
       "3           5.0  0.0   1.0      0.0  1.0\n",
       "4           0.0  5.0   0.0      0.0  0.0\n",
       "9           7.0  0.0   0.0      0.0  0.0\n",
       "14          5.0  0.0   0.0      0.0  0.0\n",
       "15          4.0  1.0   1.0      0.0  1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''prepare answer_cnt'''\n",
    "answer_cnt = df_result[['id', 'triplet_id', 'answer']].pivot_table(index='triplet_id', columns='answer', aggfunc='count').fillna(0)\n",
    "answer_cnt.columns = answer_cnt.columns.droplevel(0)\n",
    "answer_cnt = answer_cnt.rename(columns={0:'o1', 1:'o2', 2:'both', 3:'neither'})\n",
    "answer_cnt['o3'] = answer_cnt['both'] + answer_cnt['neither']\n",
    "answer_cnt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-17T04:19:42.124222Z",
     "iopub.status.busy": "2021-03-17T04:19:42.123598Z",
     "iopub.status.idle": "2021-03-17T04:19:45.383627Z",
     "shell.execute_reply": "2021-03-17T04:19:45.382270Z",
     "shell.execute_reply.started": "2021-03-17T04:19:42.124147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_0\n",
      "user_1\n",
      "user_11\n",
      "user_13\n",
      "user_15\n",
      "user_16\n",
      "user_18\n",
      "user_19\n",
      "user_2\n",
      "user_20\n",
      "user_22\n",
      "user_24\n",
      "user_25\n",
      "user_26\n",
      "user_28\n",
      "user_3\n",
      "user_30\n",
      "user_34\n",
      "user_35\n",
      "user_37\n",
      "user_38\n",
      "user_4\n",
      "user_40\n",
      "user_41\n",
      "user_5\n",
      "user_6\n",
      "user_7\n",
      "user_8\n",
      "user_9\n",
      "score    0.726913\n",
      "dtype: float64 score    0.050892\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "'''user agreement'''\n",
    "user_agree = {'user': [], 'score': []}\n",
    "for user in l_filtered_users:    \n",
    "    print(user)\n",
    "    df_answer_person = df_result[df_result['user_id'] == user].sort_values('triplet_id')\n",
    "    l_score = []\n",
    "    for i, row in df_answer_person.iterrows():\n",
    "        triplet_id = row['triplet_id']\n",
    "        if row['answer'] not in (0, 1, 2):\n",
    "            continue\n",
    "\n",
    "        answer = answer_cnt.loc[triplet_id]\n",
    "        o1, o2, o3, p, q = answer['o1'], answer['o2'], answer['o3'], answer['both'], answer['neither']\n",
    "\n",
    "        # requires more than two active votes for the triplet\n",
    "        if o1 + o2 < 2:\n",
    "            continue\n",
    "\n",
    "        if row['answer'] in {0, 1}:\n",
    "            if row['answer'] == 0:\n",
    "                oi = o1\n",
    "            else:\n",
    "                oi = o2\n",
    "            s = (oi + 0.5 * p  - 1) / (o1 + o2 + p + q - 1)\n",
    "        elif row['answer'] == 2:\n",
    "            s = (0.5 * o1 + 0.5 * o2 + p - 1 ) / (o1 + o2 + p + q - 1)\n",
    "        else:\n",
    "            s = 0\n",
    "        \n",
    "\n",
    "        l_score.append(s)\n",
    "    score = np.mean(l_score)\n",
    "    user_agree['user'].append(user)\n",
    "    user_agree['score'].append(score)\n",
    "user_agree = pd.DataFrame(user_agree)    \n",
    "user_agree.head()\n",
    "print(user_agree.mean(), user_agree.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agreement between algorithm and human\n",
    "\n",
    "Let  \n",
    "$o1, o2$ : the number of times the first (resp. second) image was chosen.  \n",
    "$p$ : the number of times \"both\" icon were chosen  \n",
    "$q$ : the nubmer of times \"neither\" icon of images were chosen\n",
    "\n",
    "\n",
    "If an algorithm choses the first or the second image, it recieves the following agreement score  \n",
    "$$\n",
    "s = \\frac{o_i + 0.5 p - 1}{o_1 + o_2 + p + q - 1}\n",
    "$$\n",
    "where $o_i$ $i\\in\\{1,2\\}$ is the choice of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_similarity_data(algo, df_triplet):\n",
    "    '''pre-fetch similarity scores'''\n",
    "    d_sim = {}\n",
    "    result_path = '/data/project/rw/viewer_CBIR/viewer/vg_coco_results/'\n",
    "    for query_id, by_qid in df_triplet.groupby('query_id'):\n",
    "        l_target_ids = list(by_qid['target_id1']) + list(by_qid['target_id2'])\n",
    "        \n",
    "        data = pd.read_csv(os.path.join(result_path, algo, f'{query_id}.tsv'), delimiter='\\t', header=None)\n",
    "        data = data.rename(columns={0:'id', 1:'sim'}).set_index('id')\n",
    "#         sim = data.loc[[int(s) for s in l_target_ids]].to_dict()['sim']\n",
    "        sim = data.reindex([int(s) for s in l_target_ids]).to_dict()['sim']\n",
    "        d_sim[query_id] = sim\n",
    "    return d_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_result(algo, query_id, l_target_ids):\n",
    "    result_path = '/data/project/rw/viewer_CBIR/viewer/vg_coco_results/'\n",
    "    data = pd.read_csv(os.path.join(result_path, algo, f'{query_id}.tsv'), delimiter='\\t', header=None)\n",
    "    data = data.rename(columns={0:'id', 1:'sim'}).set_index('id')\n",
    "    return data.loc[[int(s) for s in l_target_ids]].to_dict()['sim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 1/2 [00:03<00:03,  3.67s/it]\u001b[A\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.67s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# l_algorithms = ['sbert_rerank100', 'vg_gin_big_epoch_24', 'boo_tf', 'vg_gcn_epoch_24', 'resnet', 'gwl_gen_wordonly_rerank', 'gencap_sbert', 'vg_resnet_reg_epoch_257']\n",
    "l_algorithms = ['vg_gcn_noatt_epoch_24', 'vg_gcn_randrel_epoch_24']\n",
    "# l_algorithms = ['gwl_gen_wordonly_rerank']\n",
    "algo_scores = {'triplet_id': list(answer_cnt.index)}\n",
    "d_resnet_sim = read_similarity_data('resnet', df_triplet)\n",
    "for algo in tqdm(l_algorithms):\n",
    "    d_sim_result = read_similarity_data(algo, df_triplet)\n",
    "    l_score = []\n",
    "    for triplet_id, row in answer_cnt.iterrows():\n",
    "        o1, o2, o3, p, q = row['o1'], row['o2'], row['o3'], row['both'], row['neither']\n",
    "\n",
    "        if o1 + o2  < 2:\n",
    "            l_score.append(np.nan)\n",
    "            continue\n",
    "            \n",
    "        # get algorithm's prediction\n",
    "        triplet = df_triplet.loc[triplet_id]\n",
    "        sim1 = d_sim_result[triplet['query_id']][triplet['target_id1']]\n",
    "        sim2 = d_sim_result[triplet['query_id']][triplet['target_id2']]\n",
    "        \n",
    "        # process reranking\n",
    "        if (np.isnan(sim1)) and (np.isnan(sim2)):\n",
    "            sim1 = d_resnet_sim[triplet['query_id']][triplet['target_id1']]\n",
    "            sim2 = d_resnet_sim[triplet['query_id']][triplet['target_id2']]\n",
    "        elif (np.isnan(sim1)) and not (np.isnan(sim2)):\n",
    "            sim1 = - np.inf\n",
    "        elif not (np.isnan(sim1)) and (np.isnan(sim2)):\n",
    "            sim2 = - np.inf\n",
    "        \n",
    "        if sim1 > sim2:\n",
    "            oi = o1\n",
    "        else:\n",
    "            oi = o2\n",
    "        \n",
    "        s = (oi + 0.5 * p ) / (o1 + o2 + p + q )\n",
    "        l_score.append(s)\n",
    "    algo_scores[algo] = l_score\n",
    "algo_scores = pd.DataFrame(algo_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "triplet_id                 4957.963470\n",
       "vg_gcn_noatt_epoch_24         0.605820\n",
       "vg_gcn_randrel_epoch_24       0.603854\n",
       "dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "triplet_id    4995.389622\n",
       "boo_tfidf        0.593685\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "triplet_id                 4995.389622\n",
       "gwl_rerank                    0.580386\n",
       "gwl_gen_wordonly_rerank       0.590975\n",
       "dtype: float64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:00<00:04,  2.05it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:00<00:03,  2.05it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:01<00:03,  2.04it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:01<00:02,  2.04it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:02<00:02,  2.04it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:02<00:01,  2.04it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:03<00:01,  2.04it/s]\u001b[A\n",
      " 80%|████████  | 8/10 [00:03<00:00,  2.05it/s]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:04<00:00,  2.05it/s]\u001b[A\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.04it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# l_algorithms = ['sbert_rerank100', 'vg_gin_big_epoch_24', 'boo_tf', 'vg_gcn_epoch_24', 'resnet', 'gwl_gen_wordonly_rerank', 'gencap_sbert', 'vg_resnet_reg_epoch_257']\n",
    "# l_algorithms = ['matching_regression_attnode_tail_4_rerank']\n",
    "l_algorithms = [f'random_{i}' for i in range(10)]\n",
    "algo_scores = {'triplet_id': list(answer_cnt.index)}\n",
    "# d_resnet_sim = read_similarity_data('resnet', df_triplet)\n",
    "for algo in tqdm(l_algorithms):\n",
    "#     d_sim_result = read_similarity_data(algo, df_triplet)\n",
    "    l_score = []\n",
    "    for triplet_id, row in answer_cnt.iterrows():\n",
    "        o1, o2, o3, p, q = row['o1'], row['o2'], row['o3'], row['both'], row['neither']\n",
    "\n",
    "        if o1 + o2  < 2:\n",
    "            l_score.append(np.nan)\n",
    "            continue\n",
    "            \n",
    "        # get algorithm's prediction\n",
    "         \n",
    "        # get algorithm's prediction\n",
    "        triplet = df_triplet.loc[triplet_id]\n",
    "        sim1 = np.random.rand()\n",
    "        sim2 = np.random.rand()\n",
    "#         sim1 = d_sim_result[triplet['query_id']][triplet['target_id1']]\n",
    "#         sim2 = d_sim_result[triplet['query_id']][triplet['target_id2']]\n",
    "        \n",
    "        \n",
    "        \n",
    "        if sim1 > sim2:\n",
    "            oi = o1\n",
    "        else:\n",
    "            oi = o2\n",
    "        \n",
    "        s = (oi + 0.5 * p ) / (o1 + o2 + p + q )\n",
    "        l_score.append(s)\n",
    "    algo_scores[algo] = l_score\n",
    "algo_scores = pd.DataFrame(algo_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = algo_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47186033624495216"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.drop('triplet_id').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011238115494062404"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.drop('triplet_id').std()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
